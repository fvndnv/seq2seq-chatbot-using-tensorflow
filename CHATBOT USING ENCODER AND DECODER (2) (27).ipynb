{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
       " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
       " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
       " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORTING DATASET LINES AND CONVERSATIONS\n",
    "lines=open(\"movie_lines.txt\", encoding=\"utf-8\", errors=\"ignore\").read().split(\"\\n\")\n",
    "lines[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie=open(\"movie_conversations.txt\", encoding=\"utf-8\", errors=\"ignore\").read().split(\"\\n\")\n",
    "movie[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING DICTIONARY LINEID WITH LINE NUMBER \n",
    "id2line={}\n",
    "for line in lines:\n",
    "    _line=line.split(\" +++$+++\")\n",
    "    if len(_line)==5:\n",
    "        id2line[_line[0]]=_line[4]\n",
    "        \n",
    "#id2line will have key, value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['L194', 'L195', 'L196', 'L197'],\n",
       " ['L198', 'L199'],\n",
       " ['L200', 'L201', 'L202', 'L203']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MAKING LIST OF CONVERSATIONS\n",
    "conversation_list=[]\n",
    "for conversations in movie:\n",
    "    _conversation=conversations.split(\" +++$+++ \")[-1][1:-1].replace(\"'\", \"\").replace(\" \",\"\")\n",
    "    conversation_list.append(_conversation.split(','))\n",
    "    \n",
    "conversation_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n",
       " \" Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
       " ' Not the hacking and gagging and spitting part.  Please.',\n",
       " \" You're asking me out.  That's so cute. What's your name again?\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WE WILL MAKE SEPARATE LIST FOR QUESTIONS AS WELL AS FOR ANSWERS QITH CORRRESPOND TO EACH OTHER\n",
    "questions=[]\n",
    "answers=[]\n",
    "for conversation in conversation_list:\n",
    "    for i in range(len(conversation) - 1):\n",
    "        questions.append(id2line[conversation[i]])\n",
    "        answers.append(id2line[conversation[i+1]])\n",
    "\n",
    "questions[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
       " ' Not the hacking and gagging and spitting part.  Please.',\n",
       " \" Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\",\n",
       " ' Forget it.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DECLARIING FUNCTION FOR TEXT CLEANING\n",
    "def clean_text(text):\n",
    "    text=text.lower()\n",
    "    text=re.sub(r\"i'am\", \"i am\", text)\n",
    "    text=re.sub(r\"he's\",\"he is\", text)\n",
    "    text=re.sub(r\"she's\",\"she is\", text)\n",
    "    text=re.sub(r\"that's\",\"that is\", text)\n",
    "    text=re.sub(r\"what's\",\"ehat is\", text)\n",
    "    text=re.sub(r\"\\'ll\", \"will\", text)\n",
    "    text=re.sub(r\"\\'ve\", \"have\", text)\n",
    "    text=re.sub(r\"\\'re\", \"are\", text)\n",
    "    text=re.sub(r\"\\'d\",\"would\",text)\n",
    "    text=re.sub(r\"won't\",\"will not\", text)\n",
    "    text=re.sub(r\"can't\",\"can not\", text)\n",
    "    text=re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\",\"\" , text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again',\n",
       " ' well i thought wewould start with pronunciation if that is okay with you',\n",
       " ' not the hacking and gagging and spitting part  please',\n",
       " ' youare asking me out  that is so cute ehat is your name again']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#APPLYING FUNCTION ON QUESTION AND ANSWERS\n",
    "clean_questions=[]\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "    \n",
    "clean_questions[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' well i thought wewould start with pronunciation if that is okay with you',\n",
       " ' not the hacking and gagging and spitting part  please',\n",
       " \" okay then how 'bout we try out some french cuisine  saturday  night\",\n",
       " ' forget it']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#APPLYING FUNCTION FOR ANSWERS LIST\n",
    "clean_answers=[]\n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))\n",
    "    \n",
    "clean_answers[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COUNT WORD FREQUENCIES OF WORDS FOR QUESTIONS AND ANSWERS\n",
    "\n",
    "word2count={}\n",
    "for i in clean_questions:\n",
    "    for j in i.split():\n",
    "        if j not in word2count:\n",
    "            word2count[j]=1\n",
    "        else:\n",
    "            word2count[j]+=1\n",
    "            \n",
    "for i in clean_answers:\n",
    "    for j in i.split():\n",
    "        if j not in word2count:\n",
    "            word2count[j]=1\n",
    "        else:\n",
    "            word2count[j]+=1\n",
    "            \n",
    "#word2count.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE WILL REMOVE WORDS WHICH ACCOURS LESS THAN 10 TIMES AND ADD IF IT OCUR MORE THAN 10 TIMES\n",
    "#& ASSIGNED EACH WORD WHICH OCCUR MORE THAN 10 TIMES TO UNIQUE INTEGER\n",
    "\n",
    "threshold=10\n",
    "questionwords2int={}\n",
    "word_number=0\n",
    "for word, count in word2count.items():\n",
    "    if count > threshold:\n",
    "        questionwords2int[word]=word_number\n",
    "        word_number+=1\n",
    "        \n",
    "answerwords2int={}\n",
    "word_number=0\n",
    "for word, count in word2count.items():\n",
    "    if count > threshold:\n",
    "        answerwords2int[word]=word_number\n",
    "        word_number+=1\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WE WILL ADD 4 TOKENS WHICH ARE VERY ESSENTIAL FOR AUTOENCODERS(ENCODER AND DECODER)\n",
    "#1.PAD\n",
    "#2.SOS\n",
    "#3.EOS\n",
    "#4.OUT\n",
    "\n",
    "#PAD->>FOR MAKING SAME LENTH FOR ENCODER AND DECODER CONTEXT VECTOR\n",
    "\n",
    "#OUT->> FOR REPLACING ALL WORDS WHICH OCCUR LESS THAN 10 TIMES WITH ONE WORD \"OUT\" token\n",
    "\n",
    "#SOS->>FOR START OF SENTENCE IN DECODING \n",
    "\n",
    "#EOS->> FOR END OF SENTENCE IN DECODING \n",
    "\n",
    "#FINALLY WE WILL THESE $ TOKENS IN BOTH LIST QUESTION AND ANSWERS AND WLL GIVE ALSO\n",
    "# UNIQUE INTEGER NUMBER\n",
    "\n",
    "tokens=['<PAD>','<EOS>','<OUT>','<SOS>']\n",
    "for token in tokens:\n",
    "    questionwords2int[token]=len(questionwords2int) + 1\n",
    "\n",
    "for token in tokens:\n",
    "    answerwords2int[token]=len(answerwords2int) +1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\"DOUBT\"####################################\n",
    "\n",
    "# WILL AMKE INVERSE ANSWERWORDS2INT DICTIONARY WHICH WILL BE USEFUL FOR LATER IN SEQ2SEQ MODEL\n",
    "# here w_i and w are the keywords parameters\n",
    "\n",
    "#1.w_i->>words_integer\n",
    "#2.w->>words\n",
    "\n",
    "answerint2words={w_i:w for w, w_i in answerwords2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD <EOS> TO THE END OF EACH SENTENCE IN ORDER TO GET BETTER UNDERSTANDING FOR DECODER\n",
    "\n",
    "for i in range(len(clean_answers)):\n",
    "    clean_answers[i]+=\" <EOS>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' well i thought wewould start with pronunciation if that is okay with you <EOS>',\n",
       " ' not the hacking and gagging and spitting part  please <EOS>',\n",
       " \" okay then how 'bout we try out some french cuisine  saturday  night <EOS>\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_answers[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT EACH WORDS  OF QUESTION AND ANSERS INTO INTEGERS.\n",
    "# REPLACE WORDS WHICH ARE LESS THAN THRESHOLD WITH TOKEN <\"OUT\">\n",
    "\n",
    "question_into_integers=[]\n",
    "for question in clean_questions:\n",
    "    ints=[]\n",
    "    for word in question.split():\n",
    "        if word not in questionwords2int:\n",
    "            ints.append(questionwords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(questionwords2int[word])\n",
    "    question_into_integers.append(ints)\n",
    "    \n",
    "\n",
    "answer_into_integers=[]\n",
    "for answer in clean_answers:\n",
    "    ints=[]\n",
    "    for word in answer.split():\n",
    "        if word not in answerwords2int:\n",
    "            ints.append(answerwords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(answerwords2int[word])\n",
    "    answer_into_integers.append(ints)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[18, 19, 20, 21, 22, 23, 13628, 24, 25, 26, 27, 23, 28, 13627],\n",
       " [29, 16, 30, 5, 13628, 5, 31, 32, 33, 13627],\n",
       " [27, 79, 104, 1680, 1, 944, 37, 527, 417, 2966, 224, 259, 13627]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_into_integers[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  13628,\n",
       "  13628,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  13628,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  13628,\n",
       "  17],\n",
       " [18, 19, 20, 21, 22, 23, 13628, 24, 25, 26, 27, 23, 28]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_into_integers[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_into_integers[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  13628,\n",
       "  13628,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  13628,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  13628,\n",
       "  17],\n",
       " [18, 19, 20, 21, 22, 23, 13628, 24, 25, 26, 27, 23, 28],\n",
       " [29, 16, 30, 5, 13628, 5, 31, 32, 33],\n",
       " [34, 35, 36, 37, 25, 26, 38, 39, 40, 26, 41, 42, 17]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_into_integers[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I WILL SORT QUESTIONS AND ANSWERS BY THE SPECIFIC OPTIMUM LENGTHS FOR BETTER RESULTS AND LESS LOSS\n",
    "# I WILL SET LENGTH = 35 IN THIS CASE\n",
    "\n",
    "sorted_question_list=[]\n",
    "sorted_answer_list=[]\n",
    "\n",
    "for length in range(1, 35 + 1):\n",
    "    for i in enumerate(question_into_integers):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_question_list.append(question_into_integers[i[0]])\n",
    "            sorted_answer_list.append(answer_into_integers[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[18, 19, 20, 21, 22, 23, 13628, 24, 25, 26, 27, 23, 28, 13627],\n",
       " [29, 16, 30, 5, 13628, 5, 31, 32, 33, 13627]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_into_integers[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[52], [67], [130], [155], [142], [43], [186], [43], [193], [194]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_question_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[52], [67], [130]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_question_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[16,\n",
       "  53,\n",
       "  26,\n",
       "  52,\n",
       "  54,\n",
       "  55,\n",
       "  16,\n",
       "  56,\n",
       "  57,\n",
       "  49,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  57,\n",
       "  61,\n",
       "  45,\n",
       "  62,\n",
       "  19,\n",
       "  0,\n",
       "  29,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  13627],\n",
       " [68,\n",
       "  69,\n",
       "  65,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  65,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  65,\n",
       "  84,\n",
       "  85,\n",
       "  57,\n",
       "  80,\n",
       "  86,\n",
       "  87,\n",
       "  13627],\n",
       " [109, 13627]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_answer_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING SEQUENCE TO SEQUENCE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TENSORFLOW\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE WILL USE TENSORFLOW \n",
    "\n",
    "# WE WILL DEFINE INPUTS THARE NECCESSARY IN TNSORFLOW\n",
    "\n",
    "# CREATE TENSOR PLACEHOLDERS of ALL BELOW VARIABLE ATLEAT\n",
    "#1.INPUT\n",
    "#2.TARGET VARIABLE(\"datatype\", \"shape of input data as well as target\", \"name for placeholder\")\n",
    "#3.LEARNING RATE\n",
    "#4.KEEP_PROB_RATE THAT CONTROLS DROP OUT RATE IN TENSORFLOW\n",
    "\n",
    "def model_inputs():\n",
    "    inputs=tf.placeholder(tf.int32, [None, None], name=\"input\")\n",
    "    targets=tf.placeholder(tf.int32, [None, None], name=\"target\")\n",
    "    learning_rates=tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "    keep_prob=tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    return inputs, targets, learning_rates, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "################AS DECODING LAYER EXPECT SOE FORMAT FOR TAKING INPUTS:->>>.>################\n",
    "# SO WE WILL HAVE TO PREPROCESS DATA FIRST OF ALL\n",
    "# WE WILL MAKE FUNCTION FOR THIS:\n",
    "# WE WILL LEAVE LAST COLUMNS OF ANSWER(\"TARGET VARIABLE\") AND WILL ADD \"<SOS>\" ON STARTING OF EACH \n",
    "# - ANSWER AND WLL LET THE DECODER TO GUESS THE LAST WORD\n",
    "# WE NEED TWO THINGS FOR TAKING DATA INTO CORRECT FORMAT\n",
    "#1.BATCH_SIZE->> DECODER LAYER WORKS IN BATCH SIZE NOT INDIVIDUALLY.\n",
    "#2.IT REQUIRES <SOS> AT THE STARTING OF ANSWER(\"TARGET VARIABLE\") SOE WE HAVE TO ADD IT IN FRONT\n",
    "# OF EACH ROW OF BATCH\n",
    "######## WE WILL AMKE FUNCTION PREPROCESSED TEXT--(\"PREPROCESSED_TARGET\")--->>>>\n",
    "\n",
    "# left=(matrix for <SOS>, word2int dictionary)\n",
    "# right=(input, starting position, end position, stride of 1 by 1).\n",
    "\n",
    "\n",
    "def preprocessed_targets(target, word2int, batch_size):\n",
    "    left=tf.fill([batch_size, 1], word2int['<SOS>'])\n",
    "    right=tf.strided_slice(target, [0, 0],[batch_size, -1], [1,1])\n",
    "    preprocessed_target=tf.concat([left,right], 1)\n",
    "    return preprocessed_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE ENCODER LAYER AND WILL USE BIDIRECTIONAL RNN AND WILL USE VARIOUS LIBRARY OF TENSORFLOW\n",
    "# AND PARAMETERS\n",
    "# rnn_inputs=model inputs\n",
    "# rnn_size=number of neurons\n",
    "# keep_prob=it is used to control drop rates.\n",
    "# sequence_length=list of length of questions of per batch_size.\n",
    "#num_layers=number of layers.\n",
    "\n",
    "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "###### INITIALIZING SIMPLE RNN LAYER#################################################\n",
    "    lstm=tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "########### MAKING DROPOUT WITH ITS CONTROLLING PARAMETER############################\n",
    "    lstm_dropout=tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob=keep_prob)\n",
    "######### MAKE ENCODER CELL (\"INTERNAL MAKE MULTIRNNS\")############################\n",
    "    encoder_cell=tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "###### USING BIDIRECTIONAL RNN(FORWARD LAYER AND BACKWARD LAYER ALSO)################\n",
    "    encoder_output, encoded_state=tf.nn.Bidirectional_dynamic_rnn(cell_for=encoder_cell,\n",
    "                                                 cell_backward=encoder_cell,\n",
    "                                                 sequence_lenth=sequence_length,\n",
    "                                                 inputs=rnn_inputs,\n",
    "                                                 dtype=tf.float32)\n",
    "#### WE WILL USE ONE OF THIS OUTPUT AS \"encoder-state\".\n",
    "    return encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## \"\"DECODING THE TRAINING SET\"\" ############################################\n",
    "### MAKE ATTENTION_KEYS, ATTENTION_STATE, ATTENTION_SCORE_FUNCTION, ATTENTION CONSTRUCT FUNCTION ####\n",
    "\n",
    "#1.ATTENTION KEYS-> keys to be compared with target_state.\n",
    "#2.ATTENTION VALUES-> USED TO CONSTRUCT CONTEXT VECTOR(RETURNED BY ENCODER AND WILL BE USED BY DECODER\n",
    "#AS FIRST ELEMENT OF DECODING)\n",
    "\n",
    "#3.ATTENTION_SCORE_FUNCTION->USED TO COMPUTE SIMILARITY B/W KEYS AND TARGET_STATE.\n",
    "#4.ATTENTION_CONSTRUCT_FUINCTION-> USED TO BUILT ATTENTION STATE\n",
    "### WE WILL USE ONE OF THREE OUTPUT WHICH IS DECODER_OUTPUT ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_training_set(encoder_state,decoder_cell,decoder_embedded_input,sequence_length,decoding_scope,output_function,keep_prob,batch_size):\n",
    "    attention_state=tf.zeros([batch_size,1,decoder_cell.output_size])\n",
    "    attention_keys,attention_values,attention_score_function,attention_construct_function=tf.contrib.seq2seq.prepare_attention(\n",
    "                                                                                                               attention_states,\n",
    "                                                                                                               attention_options='bahdanau',\n",
    "                                                                                                               num_units=decoder_cell.output_size)\n",
    "    training_decoder_function=tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                           attention_keys,\n",
    "                                                                           attention_values,\n",
    "                                                                           attention_score_function,\n",
    "                                                                           attention_construct_function,\n",
    "                                                                           name='atte_dec_train')\n",
    "    decoder_output,decoder_final_state,decoder_final_context=tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cel,\n",
    "                                                                                                   training_decoder_function,\n",
    "                                                                                                   sequence_lenth,\n",
    "                                                                                                   decoder_embedded_input,\n",
    "                                                                                                   scope=decoding_sope)\n",
    "    decoder_output_dropout=tf.nn.dropout(decoder_output,keep_prob)\n",
    "    return output_function(decoder_output_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-20-ff4de18bd3a2>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-20-ff4de18bd3a2>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    attention_options='bahdanau',\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "########################## \"\"DECODING THE TRAINING SET\"\" ############################################\n",
    "#def decoding_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length,output_function,batch_size, decoding_scope, keep_prob):\n",
    "    #attention_states=tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    \n",
    "### MAKE ATTENTION_KEYS, ATTENTION_STATE, ATTENTION_SCORE_FUNCTION, ATTENTION CONSTRUCT FUNCTION ####\n",
    "\n",
    "#1.ATTENTION KEYS-> keys to be compared with target_state.\n",
    "#2.ATTENTION VALUES-> USED TO CONSTRUCT CONTEXT VECTOR(RETURNED BY ENCODER AND WILL BE USED BY DECODER\n",
    "#AS FIRST ELEMENT OF DECODING)\n",
    "\n",
    "#3.ATTENTION_SCORE_FUNCTION->USED TO COMPUTE SIMILARITY B/W KEYS AND TARGET_STATE.\n",
    "#4.ATTENTION_CONSTRUCT_FUINCTION-> USED TO BUILT ATTENTION STATE\n",
    "    #attention_keys, attention_values, attention_score_function, attention_construct_function=tf.contrib.seq2seq.prepare_attention(attention_states,\n",
    "                                                                                                                                  attention_options='bahdanau',\n",
    "                                                                                                                                  num_units=decoder_cell.output_size)\n",
    "   # training_decoder_function=tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state=[0],\n",
    "                                                                            attention_keys,\n",
    "                                                                            attention_values,\n",
    "                                                                            attention_score_function,\n",
    "                                                                            attention_construct_function,\n",
    "                                                                            name='atte_dec_train')\n",
    "### WE WILL USE ONE OF THREE OUTPUT WHICH IS DECODER_OUTPUT ##########################\n",
    "    #decoder_output, decoder_final_state, decoder_final_context=tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                     training_decoder_function,\n",
    "                                                                                                     sequence_length,\n",
    "                                                                                                     decoded_embedded_input,\n",
    "                                                                                                     scope=decoding_scope)\n",
    "                                                                                                     \n",
    "    #decoder_output_dropout=tf.nn.dropout(decoder_output, keep_prob)\n",
    "   # return output_function(decoder_otput_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## DECDONG TEST SET #####################################\n",
    "def decode_test_set(encoder_state,decoder_cell, decoder_embeddings_matrix, sos_id,eos_id, maximum_length, num_word,\n",
    "                   sequence_length, decoding_scope, output_function,batch_size, keep_prob):\n",
    "    attention_states=tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function,attention_construct_function=tf.contrib.seq2seq.prepare_attention(\n",
    "                                                                  attention_states, attention_options='bahdanau', num_units=\n",
    "                                                                                                       decoder_cell.output_size)\n",
    "    \n",
    "    test_decoder_function=tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
    "                                                                            encoder_state[0],\n",
    "                                                                            attention_keys,\n",
    "                                                                            attention_values,\n",
    "                                                                            attention_score_function,\n",
    "                                                                            attention_construct_function,\n",
    "                                                                            decoder_embeddings_matrix,\n",
    "                                                                            sos_id,\n",
    "                                                                            eos_id,\n",
    "                                                                            num_word,\n",
    "                                                                            maximum_length,\n",
    "                                                                            name='dec_atte_test')\n",
    "                                                                           \n",
    "    test_predictions,decoder_final_state,decoder_final_context=tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                   test_decoder_function,\n",
    "                                                                                                   scope=decoding_scope)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_rnn(decoder_embedded_input, decoder_embedded_matrix,encoder_state,num_words,sequence_length,rnn_size,num_layers,\n",
    "                                                                                                                   word2int,\n",
    "                                                                                                                   keep_prob,\n",
    "                                                                                                                   batch_size):\n",
    "    \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
